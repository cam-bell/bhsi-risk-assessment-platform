from typing import List, Dict, Any
from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks
from sqlalchemy.orm import Session
from app.api import deps
from app.models.company import Company
from app.schemas.company import CompanyCreate, CompanyResponse, CompanyAnalysis
from app.agents.search.streamlined_orchestrator import (
    StreamlinedSearchOrchestrator
)
from app.agents.analysis.optimized_hybrid_classifier import (
    OptimizedHybridClassifier
)
from app.agents.analytics.analytics_service import AnalyticsService
from app.crud.company import company as company_crud
from app.crud.assessment import assessment as assessment_crud
import logging
from app.core.config import settings
import uuid
import time
import datetime

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

logger.info(f"company_crud type: {type(company_crud)}")
logger.info(f"company_crud dir: {dir(company_crud)}")



router = APIRouter()


@router.post("/analyze", response_model=CompanyAnalysis)
async def analyze_company(
    company: CompanyCreate,
    background_tasks: BackgroundTasks,
    db: Session = Depends(deps.get_db)
) -> Dict[str, Any]:
    """
    UNIFIED COMPANY ANALYSIS ENDPOINT - Comprehensive Risk Assessment
    
    This endpoint combines the best of both worlds:
    1. Comprehensive data collection (BOE + NewsAPI + RSS feeds)
    2. Optimized hybrid classification (90%+ keyword efficiency)
    3. Business intelligence and analytics integration
    4. Database persistence and assessment tracking
    
    **Data Sources:**
    - BOE (Spanish official gazette)
    - NewsAPI (International news)
    - RSS feeds (El PaÃ­s, ExpansiÃ³n, El Mundo, ABC, La Vanguardia, etc.)
    
    **Business Features:**
    - Complete risk assessment with business categories
    - Analytics integration for enhanced insights
    - Database persistence for historical tracking
    - Performance monitoring and optimization
    """
    # Initialize streamlined components
    search_orchestrator = StreamlinedSearchOrchestrator()
    classifier = OptimizedHybridClassifier()
    analytics_service = AnalyticsService()
    
    try:
        # STEP 1: COMPREHENSIVE DATA COLLECTION
        # Configure which agents to use based on request parameters
        active_agents = []
        if company.include_boe:
            active_agents.append("boe")
        if company.include_news:
            active_agents.append("newsapi")
        if company.include_rss:
            # Add all RSS news sources for comprehensive coverage
            active_agents.extend([
                "elpais", "expansion", "elmundo", "abc", "lavanguardia", 
                "elconfidencial", "eldiario", "europapress"
            ])
        
        if not active_agents:
            raise HTTPException(
                status_code=400,
                detail="At least one source (BOE, news, or RSS) must be enabled"
            )
        
        # Perform comprehensive search across all enabled sources
        search_results = await search_orchestrator.search_all(
            query=company.name,
            start_date=company.start_date,
            end_date=company.end_date,
            days_back=company.days_back,
            active_agents=active_agents
        )
        
        # STEP 2: COMPREHENSIVE CLASSIFICATION
        # Classify all results using optimized hybrid classifier
        classified_results = []
        high_risk_count = 0
        
        # Process BOE results
        boe_results = search_results.get("boe", {}).get("results", [])
        for result in boe_results:
            classification = await classifier.classify_document(
                text=result.get("texto_completo", ""),
                title=result.get("titulo", ""),
                source="BOE",
                section=result.get("seccion_codigo", "")
            )
            
            result["risk_level"] = classification["label"]
            result["confidence"] = classification["confidence"]
            result["method"] = classification["method"]
            
            if classification["label"] == "High-Legal":
                high_risk_count += 1
            
            classified_results.append(result)
        
        # Process NewsAPI results
        news_results = search_results.get("newsapi", {}).get("articles", [])
        for article in news_results:
            classification = await classifier.classify_document(
                text=article.get("content", article.get("description", "")),
                title=article.get("title", ""),
                source="News",
                section=article.get("source", "")
            )
            
            article["risk_level"] = classification["label"]
            article["confidence"] = classification["confidence"]
            article["method"] = classification["method"]
            
            if classification["label"] == "High-Legal":
                high_risk_count += 1
            
            classified_results.append(article)
        
        # Process RSS results (all individual RSS agents)
        rss_results = []
        rss_agents = ["elpais", "expansion", "elmundo", "abc", 
                     "lavanguardia", "elconfidencial", "eldiario", "europapress"]
        
        for agent_name in rss_agents:
            if agent_name in search_results and search_results[agent_name].get("articles"):
                for article in search_results[agent_name]["articles"]:
                    classification = await classifier.classify_document(
                        text=article.get("content", article.get("description", "")),
                        title=article.get("title", ""),
                        source=f"RSS-{agent_name.upper()}"
                    )
                    
                    article["risk_level"] = classification["label"]
                    article["confidence"] = classification["confidence"]
                    article["method"] = classification["method"]
                    
                    if classification["label"] == "High-Legal":
                        high_risk_count += 1
                    
                    classified_results.append(article)
                    rss_results.append(article)
        
        # Determine overall risk based on findings
        total_results = len(classified_results)
        if total_results == 0:
            overall_risk = "green"
        elif high_risk_count > 0:
            overall_risk = "red"
        elif any(r.get("risk_level") == "Medium-Legal" 
                for r in classified_results):
            overall_risk = "orange"
        else:
            overall_risk = "green"
        
        # STEP 3: BUSINESS INTELLIGENCE & ANALYTICS
        # Get performance stats
        performance_stats = classifier.get_performance_stats()
        
        # Get enhanced analytics if VAT is provided
        enhanced_analytics = None
        if company.vat:
            try:
                enhanced_analytics = await analytics_service.get_comprehensive_analytics(
                    company_name=company.name,
                    include_trends=True,
                    include_sectors=False
                )
            except Exception as e:
                logger.warning(f"Analytics failed for {company.name}: {e}")
        
        # Calculate comprehensive risk assessment
        bankruptcy_risk = any("concurso" in str(r).lower() 
                             for r in classified_results)
        corruption_risk = any("blanqueo" in str(r).lower() or 
                             "corrupciÃ³n" in str(r).lower() 
                             for r in classified_results)
        
        # Format comprehensive analysis response
        analysis = {
            "company_name": company.name,
            "vat": company.vat,
            "risk_assessment": {
                "overall": overall_risk,
                "legal": "red" if high_risk_count > 0 else "green",
                "turnover": "green",  # Default for streamlined system
                "shareholding": "green",  # Default for streamlined system 
                "bankruptcy": "red" if bankruptcy_risk else "green",
                "corruption": "red" if corruption_risk else "green"
            },
            "analysis_summary": {
                "total_results": total_results,
                "high_risk_results": high_risk_count,
                "boe_results": len(boe_results),
                "news_results": len(news_results),
                "rss_results": len(rss_results),
                "keyword_efficiency": performance_stats["keyword_efficiency"],
                "llm_usage": performance_stats["llm_usage"],
                "sources_searched": active_agents
            },
            "classified_results": classified_results[:10],  # Return top 10
            "performance": performance_stats,
            "enhanced_analytics": enhanced_analytics
        }
        
        # STEP 4: DATABASE PERSISTENCE
        # Save company metadata to Company table
        company_metadata = {
            "name": company.name,
            "vat": company.vat,
        }
        db_company = company_crud.get_by_name(db, name=company.name)
        if db_company:
            company_crud.update(db, db_obj=db_company, obj_in=company_metadata)
        else:
            db_company = company_crud.create(db, obj_in=company_metadata)

        # Save comprehensive assessment to BigQuery/SQLite
        assessment_dict = {
            "id": str(uuid.uuid4()),
            "company_id": getattr(db_company, "id", str(uuid.uuid4())),
            "user_id": "system",
            "turnover": analysis["risk_assessment"]["turnover"],
            "shareholding": analysis["risk_assessment"]["shareholding"],
            "bankruptcy": analysis["risk_assessment"]["bankruptcy"],
            "legal": analysis["risk_assessment"]["legal"],
            "corruption": analysis["risk_assessment"]["corruption"],
            "overall": analysis["risk_assessment"]["overall"],
            "google_results": "[]",
            "bing_results": "[]",
            "gov_results": str(boe_results),
            "news_results": str(news_results),
            "rss_results": str(rss_results),  # Include RSS results
            "analysis_summary": str(analysis["analysis_summary"]),
        }
        assessment_crud.create(db, obj_in=assessment_dict)
        
        return analysis
    
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Error analyzing company: {str(e)}"
        )


@router.post("/batch-analyze", response_model=List[CompanyAnalysis])
async def batch_analyze_companies(
    companies: List[CompanyCreate],
    background_tasks: BackgroundTasks,
    db: Session = Depends(deps.get_db)
) -> List[Dict[str, Any]]:
    """
    Analyze multiple companies in batch using streamlined system
    """
    results = []
    for company in companies:
        try:
            analysis = await analyze_company(company, background_tasks, db)
            results.append(analysis)
        except Exception as e:
            results.append({
                "error": f"Error analyzing {company.name}: {str(e)}",
                "company": company.name
            })
    
    return results


@router.get("/{company_id}/analysis", response_model=CompanyAnalysis)
async def get_company_analysis(
    company_id: int,
    db: Session = Depends(deps.get_db)
) -> Dict[str, Any]:
    """
    Get analysis results for a specific company
    """
    company = company_crud.get(db, id=company_id)
    if not company:
        raise HTTPException(
            status_code=404,
            detail="Company not found"
        )
    
    return {
        "risk_scores": {
            "turnover": company.turnover,
            "shareholding": company.shareholding,
            "bankruptcy": company.bankruptcy,
            "legal": company.legal,
            "corruption": company.corruption,
            "overall": company.overall
        },
        "processed_results": {
            "google_results": company.google_results,
            "bing_results": company.bing_results,
            "gov_results": company.gov_results,
            "news_results": company.news_results,
            "analysis_summary": company.analysis_summary
        }
    }


@router.get("/", response_model=List[CompanyResponse])
async def list_companies(
    skip: int = 0,
    limit: int = 100,
    db: Session = Depends(deps.get_db)
) -> List[Company]:
    """
    List all companies with their risk profiles
    """
    companies = company_crud.get_multi(db, skip=skip, limit=limit)
    return companies


@router.get("/{company_name}/analytics")
async def get_company_analytics(
    company_name: str,
    include_trends: bool = True,
    include_sectors: bool = False
) -> Dict[str, Any]:
    """
    Get comprehensive analytics for a specific company using BigQuery service
    
    Args:
        company_name: Name of the company to analyze
        include_trends: Whether to include system-wide risk trends
        include_sectors: Whether to include sector analysis
    """
    try:
        analytics_service = AnalyticsService()
        analytics = await analytics_service.get_comprehensive_analytics(
            company_name=company_name,
            include_trends=include_trends,
            include_sectors=include_sectors
        )
        return analytics
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Error getting analytics for {company_name}: {str(e)}"
        )


@router.get("/analytics/trends")
async def get_risk_trends() -> Dict[str, Any]:
    """
    Get system-wide risk trends using BigQuery analytics
    """
    try:
        analytics_service = AnalyticsService()
        system_analytics = await analytics_service.get_system_analytics()
        return system_analytics.get("system_analytics", {}).get("trends", {})
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Error getting risk trends: {str(e)}"
        )


@router.get("/analytics/comparison")
async def compare_companies(
    companies: str
) -> Dict[str, Any]:
    """
    Compare risk profiles across multiple companies
    
    Args:
        companies: Comma-separated list of company names
    """
    company_list = [name.strip() for name in companies.split(",")]
    try:
        if len(company_list) < 2:
            raise HTTPException(
                status_code=400,
                detail="At least 2 companies required for comparison"
            )
        
        analytics_service = AnalyticsService()
        comparison = await analytics_service.get_risk_comparison(company_list)
        
        return comparison
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Error comparing companies: {str(e)}"
        )


@router.get("/analytics/health")
async def analytics_health_check() -> Dict[str, Any]:
    """
    Health check for analytics services
    """
    try:
        analytics_service = AnalyticsService()
        health = await analytics_service.health_check()
        
        return health
        
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e),
            "timestamp": "2024-01-17T00:00:00Z"
        }


@router.get("/system/status")
async def get_system_status():
    """
    Get status of streamlined analysis system
    """
    try:
        orchestrator = StreamlinedSearchOrchestrator()
        classifier = OptimizedHybridClassifier()
        
        # Get performance stats
        performance_stats = classifier.get_performance_stats()
        
        return {
            "status": "operational",
            "system_type": "streamlined_optimized",
            "message": ("Streamlined system with 90%+ performance "
                       "improvement active"),
            "components": {
                "search_orchestrator": "StreamlinedSearchOrchestrator",
                "classifier": "OptimizedHybridClassifier",
                "agents": list(orchestrator.agents.keys())
            },
            "performance": {
                "keyword_efficiency": performance_stats["keyword_efficiency"],
                "llm_usage": performance_stats["llm_usage"],
                "avg_processing_time": (
                    performance_stats["avg_processing_time_ms"]
                ),
                "optimization": "90%+ faster than previous system"
            },
            "capabilities": {
                "boe_search": True,
                "news_search": True,
                "hybrid_classification": True,
                "keyword_gate": True,
                "cloud_llm_fallback": True
            }
        }
    except Exception as e:
        return {
            "status": "error",
            "error": str(e),
            "message": "Could not determine system status"
        }


@router.post("/unified-analysis")
async def unified_company_analysis(
    company: CompanyCreate,
    background_tasks: BackgroundTasks,
    db: Session = Depends(deps.get_db)
) -> Dict[str, Any]:
    """
    ðŸš€ UNIFIED COMPANY ANALYSIS ENDPOINT - The Ultimate Analysis
    
    This endpoint combines ALL capabilities:
    1. Comprehensive data collection (BOE + NewsAPI + ALL RSS feeds)
    2. Optimized hybrid classification (90%+ keyword efficiency)
    3. Business intelligence and analytics integration
    4. Database persistence and assessment tracking
    5. Enhanced analytics with BigQuery integration
    6. Performance monitoring and optimization
    
    **Features:**
    - Complete data source coverage (10+ sources)
    - Business risk assessment with 6 categories
    - Enhanced analytics for companies with VAT
    - Historical tracking and trend analysis
    - Performance optimization and monitoring
    
    **Response includes:**
    - Comprehensive risk assessment
    - All classified results from all sources
    - Enhanced analytics (if VAT provided)
    - Performance metrics and optimization stats
    - Database persistence confirmation
    """
    
    # Initialize all components
    search_orchestrator = StreamlinedSearchOrchestrator()
    classifier = OptimizedHybridClassifier()
    analytics_service = AnalyticsService()
    
    overall_start_time = time.time()
    
    try:
        # STEP 1: COMPREHENSIVE DATA COLLECTION
        # Always use ALL available sources for maximum coverage
        active_agents = ["boe", "newsapi"]
        if company.include_rss:
            active_agents.extend([
                "elpais", "expansion", "elmundo", "abc", "lavanguardia", 
                "elconfidencial", "eldiario", "europapress"
            ])
        
        search_start_time = time.time()
        search_results = await search_orchestrator.search_all(
            query=company.name,
            start_date=company.start_date,
            end_date=company.end_date,
            days_back=company.days_back,
            active_agents=active_agents
        )
        search_time = time.time() - search_start_time
        
        # STEP 2: COMPREHENSIVE CLASSIFICATION
        classification_start_time = time.time()
        classified_results = []
        high_risk_count = 0
        source_counts = {}
        
        # Process all sources systematically
        for source_name, source_data in search_results.items():
            if not source_data:
                continue
                
            source_results = []
            if source_name == "boe":
                source_results = source_data.get("results", [])
            else:
                source_results = source_data.get("articles", [])
            
            source_counts[source_name] = len(source_results)
            
            for item in source_results:
                try:
                    # Determine text content based on source
                    if source_name == "boe":
                        text = item.get("texto_completo", "")
                        title = item.get("titulo", "")
                        section = item.get("seccion_codigo", "")
                    else:
                        text = item.get("content", item.get("description", ""))
                        title = item.get("title", "")
                        section = item.get("source", "")
                    
                    # Classify document
                    classification = await classifier.classify_document(
                        text=text,
                        title=title,
                        source=source_name.upper(),
                        section=section
                    )
                    
                    # Add classification to item
                    item["risk_level"] = classification["label"]
                    item["confidence"] = classification["confidence"]
                    item["method"] = classification["method"]
                    
                    if classification["label"] == "High-Legal":
                        high_risk_count += 1
                    
                    classified_results.append(item)
                    
                except Exception as e:
                    logger.warning(f"Classification failed for {source_name}: {e}")
                    # Add item with fallback classification
                    item["risk_level"] = "Unknown"
                    item["confidence"] = 0.3
                    item["method"] = "error_fallback"
                    classified_results.append(item)
        
        classification_time = time.time() - classification_start_time
        
        # STEP 3: BUSINESS INTELLIGENCE & ANALYTICS
        analytics_start_time = time.time()
        
        # Get enhanced analytics if VAT is provided
        enhanced_analytics = None
        if company.vat:
            try:
                enhanced_analytics = await analytics_service.get_comprehensive_analytics(
                    company_name=company.name,
                    include_trends=True,
                    include_sectors=True
                )
            except Exception as e:
                logger.warning(f"Enhanced analytics failed: {e}")
        
        analytics_time = time.time() - analytics_start_time
        
        # STEP 4: COMPREHENSIVE RISK ASSESSMENT
        total_results = len(classified_results)
        
        # Determine overall risk
        if total_results == 0:
            overall_risk = "green"
        elif high_risk_count > 0:
            overall_risk = "red"
        elif any(r.get("risk_level") == "Medium-Legal" 
                for r in classified_results):
            overall_risk = "orange"
        else:
            overall_risk = "green"
        
        # Calculate specific risk categories
        bankruptcy_risk = any("concurso" in str(r).lower() 
                             for r in classified_results)
        corruption_risk = any("blanqueo" in str(r).lower() or 
                             "corrupciÃ³n" in str(r).lower() 
                             for r in classified_results)
        
        # STEP 5: DATABASE PERSISTENCE
        db_start_time = time.time()
        
        # Save company metadata
        company_metadata = {
            "name": company.name,
            "vat": company.vat,
        }
        db_company = company_crud.get_by_name(db, name=company.name)
        if db_company:
            company_crud.update(db, db_obj=db_company, obj_in=company_metadata)
        else:
            db_company = company_crud.create(db, obj_in=company_metadata)
        
        # Save comprehensive assessment
        assessment_dict = {
            "id": str(uuid.uuid4()),
            "company_id": getattr(db_company, "id", str(uuid.uuid4())),
            "user_id": "system",
            "turnover": "green",  # Default
            "shareholding": "green",  # Default
            "bankruptcy": "red" if bankruptcy_risk else "green",
            "legal": "red" if high_risk_count > 0 else "green",
            "corruption": "red" if corruption_risk else "green",
            "overall": overall_risk,
            "google_results": "[]",
            "bing_results": "[]",
            "gov_results": str(search_results.get("boe", {})),
            "news_results": str(search_results.get("newsapi", {})),
            "rss_results": str({k: v for k, v in search_results.items() 
                              if k not in ["boe", "newsapi"]}),
            "analysis_summary": str({
                "total_results": total_results,
                "high_risk_results": high_risk_count,
                "source_counts": source_counts,
                "rss_sources": [k for k in search_results.keys() 
                              if k not in ["boe", "newsapi"]]
            }),
        }
        assessment_crud.create(db, obj_in=assessment_dict)
        
        db_time = time.time() - db_start_time
        
        # STEP 6: COMPREHENSIVE RESPONSE ASSEMBLY
        total_time = time.time() - overall_start_time
        performance_stats = classifier.get_performance_stats()
        
        response = {
            "company_name": company.name,
            "vat": company.vat,
            "search_date": datetime.datetime.now().isoformat(),
            "date_range": {
                "start": company.start_date,
                "end": company.end_date,
                "days_back": company.days_back
            },
            "risk_assessment": {
                "overall": overall_risk,
                "legal": "red" if high_risk_count > 0 else "green",
                "turnover": "green",
                "shareholding": "green",
                "bankruptcy": "red" if bankruptcy_risk else "green",
                "corruption": "red" if corruption_risk else "green"
            },
            "analysis_summary": {
                "total_results": total_results,
                "high_risk_results": high_risk_count,
                "source_counts": source_counts,
                "keyword_efficiency": performance_stats["keyword_efficiency"],
                "llm_usage": performance_stats["llm_usage"],
                "sources_searched": active_agents
            },
            "classified_results": classified_results[:20],  # Top 20 results
            "enhanced_analytics": enhanced_analytics,
            "performance": {
                **performance_stats,
                "total_time_seconds": f"{total_time:.2f}",
                "search_time_seconds": f"{search_time:.2f}",
                "classification_time_seconds": f"{classification_time:.2f}",
                "analytics_time_seconds": f"{analytics_time:.2f}",
                "database_time_seconds": f"{db_time:.2f}",
                "optimization": "Unified analysis with comprehensive coverage"
            },
            "database_stats": {
                "company_saved": True,
                "assessment_saved": True,
                "company_id": getattr(db_company, "id", "unknown")
            }
        }
        
        return response
        
    except Exception as e:
        total_time = time.time() - overall_start_time
        logger.error(f"Unified analysis failed for {company.name}: {e}")
        return {
            "company_name": company.name,
            "vat": company.vat,
            "risk_assessment": {},
            "analysis_summary": {},
            "classified_results": [],
            "error": f"Unified analysis failed: {str(e)}",
            "performance": {
                "total_time_seconds": f"{total_time:.2f}",
                "error": "Analysis failed before completion"
            }
        } 